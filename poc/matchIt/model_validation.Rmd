---
title: "Beta to Release Matching: Model Validation"
author: "Mariana de Oliveira Santos Silva"
date: 'Last Updated: `r format(Sys.time(), "%B %d, %Y")`'
output: 
  html_document:
    theme: "flatly"
    toc: true
    toc_float: true
    code_folding: "hide"
    highlight: tango
---

# Introduction

The goal of this notebook is to validate the best model identified in the [previous work](https://github.com/godelstheory/ff-beta-release-matching/blob/marianaossilva/poc/matchIt/model_selection.html). Here, we follow two different applications:

* To balance on the other covariates (e.g., environment and performance metrics), then look at the difference in the user engagement metrics between the balanced Beta and Release for that version (N). This gives us an idea of how clients with similar environments and performance resemble Release in terms of usage. 
* To balance the Beta and Release datasets to resemble each other across the covariates we are concerned with. Balancing, in this case, yields a set of `client_id` for Beta that resembles Release. Our application is then querying the current Beta data (Version N+1) for this `client_id`, and then calculate the metrics we care about from the covariates we care about. This is our outcome.

# Loading the data

```{r message=FALSE, warning=FALSE, include=FALSE}
## Loading the needed libraries

library(kableExtra)      # help you build common complex tables and manipulate table styles
library(DataExplorer)    # automated data exploration
library(tidyverse)       # for general data wrangling (includes readr and dplyr)
library(ggplot2)         # to draw statistical plots 
library(dplyr)           # for data frame manipulation
library(MatchIt)
library(optmatch)
library(cem)
library(RItools)
library(magrittr)
library(tidyr)
library(tidyselect)
library(grt)
library(tableone)
library(cobalt)
library(funModeling)
library(cowplot)
library(plyr)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
## Loading the training dataset
#load('~/ff-beta-release-matching/poc/matchIt/feature_selection.RData')

df_train_encoder <- read.csv("df_train_encoder.csv", header = T, sep = "\t", encoding="UTF-8") 
df_validate_encoder <- read.csv("df_validate_encoder.csv", header = T, sep = "\t", encoding="UTF-8")
#df_train_encoder$default_search_engine_missing <- 0
```

```{r include=FALSE}
generate_formula <- function(tr_cov, label){
  # Focus on linear and no-interactions
  return(as.formula(paste(label, '~', paste(tr_cov, collapse="+"))))
}

calc_means <- function(df.match, ho_cov, add_1 = FALSE){
  # Validation
  ## Means: holdout
  ho_mean <- df.match %>% 
    dplyr::select(ho_cov, label) %>% 
    mutate_all(function(x) if(is.numeric(x) && add_1) x+1 else x) %>%
    group_by(label) %>% 
    summarise_all(mean)
  return(ho_mean)
}

calc_medians <- function(df.match, ho_cov, add_1 = FALSE){
  # Validation
  ## medians: holdout
  ho_median <- df.match %>% 
    dplyr::select(ho_cov, label) %>% 
    mutate_all(function(x) if(is.numeric(x) && add_1) x+1 else x) %>%
    group_by(label) %>% 
    summarise_all(median)
  return(ho_median)
}

calc_delta <- function(df.match, ho_cov){
  perc_diff <- function(x) abs(x[1]-x[2])/x[2]
  
  means <- calc_means(df.match, ho_cov) %>%
    dplyr::select(-label) %>% 
    summarise_all(perc_diff)
  
  medians <- calc_medians(df.match, ho_cov) %>%
    dplyr::select(-label) %>% 
    summarise_all(perc_diff)
  
  df <- means %>% 
    rbind(medians) %>% 
    as.data.frame() %>%
    set_rownames(c('means', 'medians'))
  return(df)
}

calc_stats <- function(df.match, ho_cov, add_1 = FALSE){
  means <- calc_means(df.match, ho_cov, add_1 = add_1) %>%
    mutate(metric = 'mean')
  medians <- calc_medians(df.match, ho_cov, add_1 = add_1) %>%
    mutate(metric = 'median')
  df <- means %>% 
    rbind(medians) %>% 
    as.data.frame()
  return(df)
}

#### Diagnostics ####
compare_log_cont <- function(df, covariate, tr_mean = NULL, tr_median = NULL, ho_mean = NULL, print = TRUE){
  p <- ggplot(df %>% mutate(!!covariate := get(covariate)+1), aes(label, get(covariate))) +
    geom_violin(aes(fill = label), alpha=0.5) +
    geom_boxplot(width=0.1) +
    scale_y_log10() + scale_fill_manual(values = c("#111d5e", "#b21f66","#611e62")) +
    theme_bw() +
    guides(fill = FALSE) +
    labs(x='Channel', y='Measure', title=covariate) + coord_flip()

  if(print) print(p)
  return(p)
}
```

# Data Preparation

## Training

```{r echo=FALSE}
# define response field
label <- 'is_release'

## Training set
kable(introduce(df_train_encoder)) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
scroll_box(width = "100%")
```

## Validation

```{r echo=FALSE}
## Validation set
kable(introduce(df_validate_encoder)) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
scroll_box(width = "100%")
```


```{r}
# sampling for beta overrepresentation
build_df <- function(df_c, multiple){
  df_beta <- df_c %>% filter(label_beta == 1)
  df_rel <- df_c %>% filter(label_beta == 0)
  n_beta <- nrow(df_beta)
  
  df <- df_rel %>%
    sample_n(size = round(n_beta / multiple)) %>%
    rbind(df_beta)
  
  return(df)
}

df_train_1x <- build_df(df_train_encoder, 1)
df_validate_1x <- build_df(df_validate_encoder, 1)
```

Let's check the class balance.

```{r include=FALSE}
df_validate_1x$label <- mapvalues(df_validate_1x$is_release, from=c(FALSE,TRUE), to=c('beta','release'))
df_validate_encoder$label <- mapvalues(df_validate_encoder$is_release, from=c(FALSE,TRUE), to=c('beta','release'))
df_train_1x$label <- mapvalues(df_train_1x$is_release, from=c(FALSE,TRUE), to=c('beta','release'))
```

```{r}
print(paste('Train (50% - 50%)'))
f <- freq(as.factor(df_train_1x$label)) # ~ 50% - 50%

print(paste('Validation (50% - 50%)'))
f <- freq(as.factor(df_validate_1x$label)) # ~ 50% - 50%
```

# First Application - Training Daset (V67)

In this application, we need to balance the two groups (Beta and Release) considering the other covariates (e.g., environment and performance metrics) and then look at the difference in user engagement metrics between the balanced Beta and Release for that version (N). The utility of this application is to inform us on how Beta is different concerning Release in user engagement, with all the other covariates being equal.

## Modeling

Setting the selected expirement from previous work.

```{r echo=TRUE}
covariates <- c('daily_num_sessions_started', 'daily_num_sessions_started_max', 'FX_PAGE_LOAD_MS_2_PARENT', 'memory_mb', 'num_active_days', 'num_addons', 'num_bookmarks', 'profile_age', 'session_length', 'session_length_max','TIME_TO_DOM_COMPLETE_MS','TIME_TO_DOM_CONTENT_LOADED_END_MS','TIME_TO_DOM_INTERACTIVE_MS','TIME_TO_LOAD_EVENT_END_MS','TIME_TO_NON_BLANK_PAINT_MS') 

engagement <- c('active_hours','active_hours_max','uri_count','uri_count_max','search_count','search_count_max','num_pages','num_pages_max','daily_max_tabs','daily_max_tabs_max','daily_unique_domains','daily_unique_domains_max','daily_tabs_opened','daily_tabs_opened_max')
```

### Match using Nearest Neighbor matching - Mahalanobis: Full Dataset

The best model from previous work.

```{r message=FALSE, warning=FALSE}
nn.ma <- matchit(formula = generate_formula(covariates, label), df_train_1x, 'nearest', distance='mahalanobis')
df_matched <- match.data(nn.ma)
df_matched$label <- mapvalues(df_matched$is_release, from=c(FALSE,TRUE), to=c('beta','release'))
print(summary(nn.ma))
```

```{r}
table_match <- CreateTableOne(vars = covariates, strata = "is_release", data = df_matched, test = FALSE)
print(table_match, smd = TRUE)
```

```{r fig.height=5, fig.width=9, message=FALSE, warning=FALSE}
love.plot(nn.ma, binary = "std", threshold = .1, colors = c("#111d5e", "#b21f66"), abs = T) + theme_bw()
```

***
### Observations

* Both table and plot show that the matching process successfully match all instances. That is, we have no unmatched (unadjusted) samples.
* The plot shows that, for adjusted cases (after matching), the standardized mean difference is relatively small for most covariates. The smallest (that is, less than or equal to $0.1$) are: `profile_age`, `memory_mb`, `num_bookmarks` and `num_active_days`. However, some have a high absolute value, for example, `num_addons`.

### Post-matching Beta-Release Difference:

```{r message=FALSE, warning=FALSE, results = 'asis'}
stats_post_mean <- calc_means(df_matched, engagement) %>%
  dplyr::select(-label) %>%
  set_rownames(c('beta (mean)', 'release (mean)'))

stats_post_median <- calc_medians(df_matched, engagement) %>%
  dplyr::select(-label) %>%
  set_rownames(c('beta (median)', 'release (median)'))

stats_post <- calc_delta(df_matched, engagement)

stats_mean <- stats_post_mean %>%
  rbind(stats_post[1, ]) %>%
  set_rownames(c('beta (mean)', 'release (mean)', 'delta (mean)'))

stats_median <- stats_post_median %>%
  rbind(stats_post[2, ]) %>%
  set_rownames(c('beta (median)', 'release (median)', 'delta (median)'))

stats <- stats_mean %>%
  rbind(stats_median)

knitr::kable(stats) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  row_spec(c(3,6), bold = T, color = "white", background = "#4D5686") %>%
  scroll_box(width = "100%")
```

### QQ-Plots

Q-Q plots are a handy tool for comparing distributions: the closer the values form a straight line, the higher
chance to come from a similar distribution. Here, we use Q-Q plots and the Kolmogorov-Smirnov test (KS) test to verify the differences between the balanced Beta and Release for that version (N).  

```{r fig.height=9, fig.width=9, message=FALSE, warning=FALSE}
par(mfrow = c(4, 2))  

df_beta <- df_matched %>% filter(label == 'beta')
df_rel <- df_matched %>% filter(label == 'release')

## QQ plot in R to compare two data samples
for (i in engagement) {
  
  # Training
  x_t <- df_beta[,i]
  y_t <- df_rel[,i]
  
  rg_t <- range(x_t, y_t, na.rm=T)
  
  test_t <- ks.test(x_t, y_t)$statistic
  test_t <- paste("KS Test = ", round(test_t, 3))

  ########
  
  title_t <- paste('V67', i, sep='\n')
  qqplot(x_t, y_t, main=title_t, xlim=rg_t, ylim=rg_t, xlab = "Beta", ylab = "Release", pch = 1)
  # mtext(test, side=3)
  text(min(x_t), (if(max(x_t) > max(y_t)) max(x_t) else max(y_t)), test_t, adj=c(0,1))
  abline(0,1, col="#fe346e", lty=2)
}
```

### Violin Plots

The following violin plots depicts distributions for the Beta and release subsets, for the version v67 (training).

```{r fig.height=5, fig.width=8, message=FALSE, warning=FALSE}
par(mfrow = c(4, 2))

## Violin plots
for (i in engagement) {
  p <- ggplot(df_matched, aes(x=label, y=eval(as.name(i)), fill=label)) + 
  geom_violin(trim=FALSE) +
  labs(title=i,x="Channel", y = "Measure") + 
  scale_y_log10() + scale_fill_manual(values=c("#111d5e", "#b21f66")) +
  geom_boxplot(width=0.1, fill="white", alpha=0.5) + 
  theme_bw() + theme(legend.position="none") + coord_flip()
  print(p)
}
```

***

### Observations

* Analysing only the Q-Q plots, the best matching variables are near the $x = y$ line (e.g., `active_hours`, `num_pages`, `search_count` and `daily_unique_domains`). However, some plots present a deviation from such a line at the high-end (e.g., `daily_max_tabs` and `daily_tabs_opened`). 
* Regarding violin plots, overall, the Beta users from V67, with all the other covariates (*experiment 3*) being equal, are quite similar concerning Release in user engagement. Only the following metrics yielded a great deviation (as already pointed by the KS test):

  - `num_pages` and `num_pages_max`
  - `daily_max_tabs` and `daily_max_tabs_max`

# Second Application - Validation dataset (V68)

In this application, we need to balance the Beta and Release datasets to resemble each other across the covariates we are concerned with, that is, the user engagement metrics. Balancing, in this case, yields a set of `client_id` for Beta that resembles Release. This gives us an idea of how these users do indeed change in time. If we see changes that are larger than anticipated, then we know that something significant is happening in user engagement that we can "forecast" in the subsequent Release.

First, we determine the number of training (v67) Beta and Release clients that are in the validation set (v68).

```{r message=FALSE, warning=FALSE, echo=FALSE}
mutual_clients <- df_train_1x[, c('client_id', 'label')] %>% 
  inner_join(df_validate_encoder[, c('client_id', 'label')]) %>%
  count('label')

mutual_clients
```

Let's compare this to existing distribution:

```{r echo=FALSE}
total <- df_train_1x %>% count('label')
cat('Percentage of beta mutual clients:',round(mutual_clients$freq[1] * 100 / total$freq[1]),'%\n')
cat('Percentage of release mutual clients:',round(mutual_clients$freq[2] * 100 / total$freq[2]),'%')
```

Hence, most training clients (65%) are in the validation set.

## Holdout Covariates

* User engagement metrics:
  - active_hours
  - active_hours_max
  - uri_count
  - uri_count_max
  - search_count
  - search_count_max
  - num_pages
  - num_pages_max
  - daily_max_tabs
  - daily_max_tabs_max
  - daily_unique_domains
  - daily_unique_domains_max
  - daily_tabs_opened
  - daily_tabs_opened_max

Subset the validation clients down to those matched:

```{r echo=FALSE}
df_validate_matched <- df_validate_1x %>%
  filter(client_id %in% df_matched$client_id)

df_validate_matched %>%
  count('label')
```

### Training and Validation Difference:

```{r}
stats_pre <- calc_delta(df_validate_1x, engagement)
stats_post <- calc_delta(df_validate_matched, engagement)

stats_mean <- stats_pre[1, ] %>%
  rbind(stats_post[1, ]) %>%
  set_rownames(c('pre-matching', 'post-matching'))

stats_median <- stats_pre[2, ] %>%
  rbind(stats_post[2, ]) %>%
  set_rownames(c('pre-matching', 'post-matching'))
```

> Mean

```{r results = 'asis'}
knitr::kable(stats_mean) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  scroll_box(width = "100%")
```

> Median

```{r results = 'asis'}
knitr::kable(stats_median) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  scroll_box(width = "100%")
```

***

```{r fig.width=10, fig.height=50, echo=FALSE, results = 'asis'}
df_validate_full <- df_validate_matched %>%
  filter(label == 'beta') %>%
  mutate(label =  'beta - matched') %>%
  rbind(df_validate_1x) %>% 
  distinct()

stats <- calc_stats(df_validate_full, engagement, add_1 = TRUE) %>%
  dplyr::select('metric','label', everything())

knitr::kable(stats) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  row_spec(1:3, bold = T, color = "white", background = "#4D5686") %>%
  row_spec(4:6, bold = T, color = "white", background = "#C5578C") %>%
  scroll_box(width = "100%")
```

```{r message=FALSE, warning=FALSE}
plots <- list()

for (covariate in engagement) {
  stats_rel <- stats %>% filter(label == 'release') %>% dplyr::select(covariate, metric)
  means <- stats_rel[stats_rel$metric == 'mean', covariate]
  medians <- stats_rel[stats_rel$metric == 'median', covariate]
  ho_means <- stats %>% filter(label == 'beta' & metric == 'mean') %>% dplyr::select(covariate)
  
  plots[[covariate]] <- compare_log_cont(df_validate_full, covariate, means, medians, as.numeric(ho_means), print=FALSE) 
}
```

```{r echo=FALSE, fig.height=50, fig.width=10, message=FALSE, warning=FALSE}
plot_grid(plotlist = plots, ncol = 1)
```

## Training Covariates

* Experiment 3:
  - daily_num_sessions_started
  - daily_num_sessions_started_max
  - FX_PAGE_LOAD_MS_2_PARENT
  - memory_mb
  - num_active_days
  - num_addons
  - num_bookmarks
  - profile_age
  - session_length
  - session_length_max
  - TIME_TO_DOM_COMPLETE_MS
  - TIME_TO_DOM_CONTENT_LOADED_END_MS
  - TIME_TO_DOM_INTERACTIVE_MS
  - TIME_TO_LOAD_EVENT_END_MS
  - TIME_TO_NON_BLANK_PAINT_MS 

```{r}
stats_pre <- calc_delta(df_validate_1x, covariates)
stats_post <- calc_delta(df_validate_matched, covariates)

stats_mean <- stats_pre[1, ] %>%
  rbind(stats_post[1, ]) %>%
  set_rownames(c('pre-matching', 'post-matching'))

stats_median <- stats_pre[2, ] %>%
  rbind(stats_post[2, ]) %>%
  set_rownames(c('pre-matching', 'post-matching'))
```

> Mean

```{r results = 'asis'}
knitr::kable(stats_mean) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  scroll_box(width = "100%")
```

> Median

```{r results = 'asis'}
knitr::kable(stats_median) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  scroll_box(width = "100%")
```

***

```{r fig.width=10, fig.height=50, echo=FALSE, results = 'asis'}
stats <- calc_stats(df_validate_full, covariates, add_1 = TRUE) %>%
  dplyr::select('metric','label', everything())

knitr::kable(stats) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  row_spec(1:3, bold = T, color = "white", background = "#4D5686") %>%
  row_spec(4:6, bold = T, color = "white", background = "#C5578C") %>%
  scroll_box(width = "100%")
```

```{r message=FALSE, warning=FALSE}
plots <- list()

for (covariate in covariates) {
  stats_rel <- stats %>% filter(label == 'release') %>% dplyr::select(covariate, metric)
  means <- stats_rel[stats_rel$metric == 'mean', covariate]
  medians <- stats_rel[stats_rel$metric == 'median', covariate]
  ho_means <- stats %>% filter(label == 'beta' & metric == 'mean') %>% dplyr::select(covariate)
  
  plots[[covariate]] <- compare_log_cont(df_validate_full, covariate, means, medians, as.numeric(ho_means), print=FALSE) 
}
```

```{r echo=FALSE, fig.height=50, fig.width=10, message=FALSE, warning=FALSE}
plot_grid(plotlist = plots, ncol = 1)
```