---
title: "Beta to Release Matching: Model Validation"
author: "Mariana de Oliveira Santos Silva"
date: 'Last Updated: `r format(Sys.time(), "%B %d, %Y")`'
output: 
  html_document:
    theme: "flatly"
    toc: true
    toc_float: true
    code_folding: "hide"
    highlight: tango
---

# Introduction

The goal of this notebook is to validate the best model identified in the [previous work](https://github.com/godelstheory/ff-beta-release-matching/blob/marianaossilva/poc/matchIt/model_selection.html). Here, we follow two different applications:

* To balance on the other covariates (e.g., environment and performance metrics), then look at the difference in the user engagement metrics between the balanced Beta and Release for that version (N). This gives us an idea of how clients with similar environments and performance resemble Release in terms of usage. 
* To balance the Beta and Release datasets to resemble each other across the covariates we are concerned with. Balancing, in this case, yields a set of `client_id` for Beta that resembles Release. Our application is then querying the current Beta data (Version N+1) for this `client_id`, and then calculate the metrics we care about from the covariates we care about. This is our outcome.

# Loading the data

```{r message=FALSE, warning=FALSE, include=FALSE}
## Loading the needed libraries

library(kableExtra)      # help you build common complex tables and manipulate table styles
library(DataExplorer)    # automated data exploration
library(tidyverse)       # for general data wrangling (includes readr and dplyr)
library(ggplot2)         # to draw statistical plots 
library(dplyr)           # for data frame manipulation
library(MatchIt)
library(optmatch)
library(cem)
library(RItools)
library(magrittr)
library(tidyr)
library(tidyselect)
library(grt)
library(tableone)
library(cobalt)
library(funModeling)
library(cowplot)
library(plyr)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
## Loading the training dataset
#load('~/ff-beta-release-matching/poc/matchIt/feature_selection.RData')

df_train_encoder <- read.csv("df_train_encoder.csv", header = T, sep = "\t", encoding="UTF-8") 
df_validate_encoder <- read.csv("df_validate_encoder.csv", header = T, sep = "\t", encoding="UTF-8")
df_train_encoder$default_search_engine_missing <- 0
```

```{r include=FALSE}
generate_formula <- function(tr_cov, label){
  # Focus on linear and no-interactions
  return(as.formula(paste(label, '~', paste(tr_cov, collapse="+"))))
}

calc_means <- function(df.match, ho_cov, add_1 = FALSE){
  # Validation
  ## Means: holdout
  ho_mean <- df.match %>% 
    dplyr::select(ho_cov, label) %>% 
    mutate_all(function(x) if(is.numeric(x) && add_1) x+1 else x) %>%
    group_by(label) %>% 
    summarise_all(mean)
  return(ho_mean)
}

calc_medians <- function(df.match, ho_cov, add_1 = FALSE){
  # Validation
  ## medians: holdout
  ho_median <- df.match %>% 
    dplyr::select(ho_cov, label) %>% 
    mutate_all(function(x) if(is.numeric(x) && add_1) x+1 else x) %>%
    group_by(label) %>% 
    summarise_all(median)
  return(ho_median)
}

calc_delta <- function(df.match, ho_cov){
  perc_diff <- function(x) abs(x[1]-x[2])/x[2]
  
  means <- calc_means(df.match, ho_cov) %>%
    dplyr::select(-label) %>% 
    summarise_all(perc_diff)
  
  medians <- calc_medians(df.match, ho_cov) %>%
    dplyr::select(-label) %>% 
    summarise_all(perc_diff)
  
  df <- means %>% 
    rbind(medians) %>% 
    as.data.frame() %>%
    set_rownames(c('means', 'medians'))
  return(df)
}

calc_stats <- function(df.match, ho_cov, add_1 = FALSE){
  means <- calc_means(df.match, ho_cov, add_1 = add_1) %>%
    mutate(metric = 'mean')
  medians <- calc_medians(df.match, ho_cov, add_1 = add_1) %>%
    mutate(metric = 'median')
  df <- means %>% 
    rbind(medians) %>% 
    as.data.frame()
  return(df)
}

#### Diagnostics ####
compare_log_cont <- function(df, covariate, tr_mean = NULL, tr_median = NULL, ho_mean = NULL, print = TRUE){
    p <- ggplot(df %>% mutate(!!covariate := get(covariate)+1), aes(label, get(covariate))) +
        geom_violin(aes(fill = label), alpha = 0.5) +
        geom_boxplot(width=0.1) +
        scale_y_log10() + scale_fill_manual(values = c("#111d5e", "#b21f66","#611e62")) +
        theme_bw() +
        guides(fill = FALSE) +
        labs(x = 'Channel', y='Measure', title=covariate) + coord_flip()
    
    # add in lines for treatment means/medians
    if (!is.null(tr_mean)){
        p <- p + 
            geom_hline(yintercept=tr_mean, linetype="solid", lwd = .5) + 
            geom_hline(yintercept=tr_median, linetype="dashed", lwd = .5) + 
            geom_hline(yintercept=ho_mean, linetype="dashed", color = "#D62728B2", lwd = 1)
    }
    
    if(print) print(p)
    return(p)
}
```

# Data Preparation

## Training

```{r echo=FALSE}
# define response field
label <- 'is_release'

## Training set
kable(introduce(df_train_encoder)) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
scroll_box(width = "100%")
```

## Validation

```{r echo=FALSE}
## Validation set
kable(introduce(df_validate_encoder)) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
scroll_box(width = "100%")
```


```{r}
# sampling for beta overrepresentation
build_df <- function(df_c, multiple){
  df_beta <- df_c %>% filter(label_beta == 1)
  df_rel <- df_c %>% filter(label_beta == 0)
  n_beta <- nrow(df_beta)
  
  df <- df_rel %>%
    sample_n(size = round(n_beta / multiple)) %>%
    rbind(df_beta)
  
  return(df)
}

df_train_1x <- build_df(df_train_encoder, 1)
df_validate_1x <- build_df(df_validate_encoder, 1)
```

Let's check the class balance.

```{r include=FALSE}
df_validate_1x$label <- mapvalues(df_validate_1x$is_release, from=c(FALSE,TRUE), to=c('beta','release'))
df_validate_encoder$label <- mapvalues(df_validate_encoder$is_release, from=c(FALSE,TRUE), to=c('beta','release'))
df_train_1x$label <- mapvalues(df_train_1x$is_release, from=c(FALSE,TRUE), to=c('beta','release'))
df_train_encoder$label <- mapvalues(df_train_encoder$is_release, from=c(FALSE,TRUE), to=c('beta','release'))
```

```{r}
print(paste('Train (50% - 50%)'))
f <- freq(as.factor(df_train_1x$label)) # ~ 50% - 50%

print(paste('Validation (50% - 50%)'))
f <- freq(as.factor(df_validate_1x$label)) # ~ 50% - 50%
```

# First Application - Training Daset (V67)

In this application, we need to balance the two groups (Beta and Release) considering the other covariates (e.g., environment and performance metrics) and then look at the difference in user engagement metrics between the balanced Beta and Release for that version (N). The utility of this application is to inform us on how Beta is different concerning Release in user engagement, with all the other covariates being equal.

## Modeling

Setting the selected expirement from previous work.

```{r echo=TRUE}
covariates <- c('daily_num_sessions_started', 'daily_num_sessions_started_max', 'FX_PAGE_LOAD_MS_2_PARENT', 'memory_mb', 'num_active_days', 'num_addons', 'num_bookmarks', 'profile_age', 'session_length', 'session_length_max','TIME_TO_DOM_COMPLETE_MS','TIME_TO_DOM_CONTENT_LOADED_END_MS','TIME_TO_DOM_INTERACTIVE_MS','TIME_TO_LOAD_EVENT_END_MS','TIME_TO_NON_BLANK_PAINT_MS') 

engagement <- c('active_hours','active_hours_max','uri_count','uri_count_max','search_count','search_count_max','num_pages','num_pages_max','daily_max_tabs','daily_max_tabs_max','daily_unique_domains','daily_unique_domains_max','daily_tabs_opened','daily_tabs_opened_max')
```

### Match using Nearest Neighbor matching: Full Dataset

The best model from previous work.

```{r message=FALSE, warning=FALSE}
nn <- matchit(formula = generate_formula(covariates, label), df_train_1x, 'nearest', replace = TRUE)
df_matched <- match.data(nn)
df_matched$label <- mapvalues(df_matched$is_release, from=c(FALSE,TRUE), to=c('beta','release'))
print(summary(nn))
```

```{r}
table_match <- CreateTableOne(vars = covariates, strata = "label", data = df_matched, test = FALSE)
print(table_match, smd = TRUE)
```

```{r fig.height=5, fig.width=9, message=FALSE, warning=FALSE}
love.plot(nn, binary = "std", threshold = .1, shapes = c("triangle filled", "circle filled"), colors = c("#111d5e", "#b21f66"), abs = F, position = "top", var.order = "unadjusted") + theme_bw()
```

***
#### Observations

* The Love plot is a summary plot of covariate balance pre and post matching. Each point represents the balance statistic for that covariate, colored based on whether it is calculated before or after adjustment. The dotted lines represent the threshold set ($0.1$); if most or all of the points after adjustment are within the threshold, that is good evidence that balance has been achieved
* The plot shows that, for unadjusted cases (pre-matching), the standardized mean difference is not relatively large (except for `num_addons`). However, for adjusted cases (post-matching), the standardized mean difference is smaller. That is, for most cases, the absolute value is even smaller than the threshold ($0.1$)
* Therefore, this result is a good evidence that the matching worked well, as the balance was improved on almost all variables after adjustment

### Post-matching Beta-Release Difference

```{r message=FALSE, warning=FALSE, results = 'asis'}
stats_post_mean <- calc_means(df_matched, engagement) %>%
  dplyr::select(-label) %>%
  set_rownames(c('beta (mean)', 'release (mean)'))

stats_post_median <- calc_medians(df_matched, engagement) %>%
  dplyr::select(-label) %>%
  set_rownames(c('beta (median)', 'release (median)'))

stats_post <- calc_delta(df_matched, engagement)

stats_mean <- stats_post_mean %>%
  rbind(stats_post[1, ]) %>%
  set_rownames(c('beta (mean)', 'release (mean)', 'delta (mean)'))

stats_median <- stats_post_median %>%
  rbind(stats_post[2, ]) %>%
  set_rownames(c('beta (median)', 'release (median)', 'delta (median)'))

stats <- stats_mean %>%
  rbind(stats_median)

knitr::kable(stats) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  row_spec(c(3,6), bold = T, color = "white", background = "#4D5686") %>%
  scroll_box(width = "100%")
```

```{r fig.width=10, fig.height=50, echo=FALSE, results = 'asis'}
df_training_full <- df_matched %>% 
  dplyr::select(-weights, -distance) %>%
  filter(label == 'beta') %>%
  mutate(label =  'beta - matched') %>%
  rbind(df_train_1x) %>% 
  distinct()

stats <- calc_stats(df_training_full, engagement) %>%
  dplyr::select('metric','label', everything())

knitr::kable(stats) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  row_spec(1:3, bold = T, color = "white", background = "#4D5686") %>%
  row_spec(4:6, bold = T, color = "white", background = "#C5578C") %>%
  scroll_box(width = "100%")
```

### Kolmogorov-Smirnov test (KS)

We can use the Kolmogorov-Smirnov test (KS) to verify the differences between the balanced Beta and Release for the v67 version. It is between 0 and 1, and represents how two data sets are similar. Smaller KS distance values indicate better balance.

```{r fig.height=9, fig.width=9, message=FALSE, warning=FALSE}
par(mfrow = c(4, 2))  

df_beta <- df_matched %>% filter(label == 'beta')
df_rel <- df_matched %>% filter(label == 'release')

output = data.frame()

for (i in engagement) {
  
  # Training
  x_t <- df_beta[,i]
  y_t <- df_rel[,i]
  
  rg_t <- range(x_t, y_t, na.rm=T)
  ks <- ks.test(x_t, y_t)$statistic
  output = rbind(output, data.frame(KS = ks))
}
```

```{r message=FALSE, warning=FALSE, echo=FALSE, results = 'asis'}
kable(set_rownames(output, engagement)) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
scroll_box(width = "100%")
```

### Visual inspection 

Here, we display density plots for the two groups on the given user engagement metric, so we can visually compare their distribution. The degree to which the densities for the two groups overlap is a good measure of group balance on the given covariate; significant differences in shape can be indicative of poor balance, even when the mean differences and variance ratios are well within thresholds.

```{r echo=FALSE, fig.height=4, fig.width=7}
par(mfrow = c(7, 2))
for (v in engagement){
  p <- ggplot(data=df_matched, aes(x = get(v), group = label, fill = label)) +
    geom_density(aes(fill = label), adjust=1.5, alpha=.4) + 
    scale_fill_manual(values = c("#111d5e", "#b21f66")) +
    labs(y='Density', title=v) + theme_bw() + theme(legend.position="bottom") 
  print(p)
}
```

The following violin plots depicts distributions for the following subsets: 

* Beta v67: pre-matching
* Beta v67: matched and subsetted
* Release v67

**NOTE**: Guiding lines have been added for the following:

* <span style="color:black">black</span> solid: Release mean
* <span style="color:black">black</span> dashed: Release median
* <span style="color:#D62728B2">red</span> dashed line: subsetted Beta _mean_. 

```{r message=FALSE, warning=FALSE}
plots <- list()

for (covariate in engagement) {
  stats_rel <- stats %>% filter(label == 'release') %>% dplyr::select(covariate, metric)
  means <- stats_rel[stats_rel$metric == 'mean', covariate]
  medians <- stats_rel[stats_rel$metric == 'median', covariate]
  ho_means <- stats %>% filter(label == 'beta' & metric == 'mean') %>% dplyr::select(covariate)
  
  plots[[covariate]] <- compare_log_cont(df_training_full, covariate, means, medians, as.numeric(ho_means), print=FALSE) 
}
```

```{r echo=FALSE, fig.height=50, fig.width=10, message=FALSE, warning=FALSE}
plot_grid(plotlist = plots, ncol = 1)
```

***
#### Observations

The density and violin plots show that there are significant differences between both groups (Beta and Release) concerning some user engagement metrics, listed as follows.

* `num_pages`
* `num_pages_max`
* `active_hours`
* `active_hours_max`
* `uri_count`
* `uri_count_max`
* `daily_unique_domains`
* `daily_unique_domains_max`

# Second Application - Validation dataset (V68)

In this application, we need to balance the Beta and Release datasets to resemble each other across the covariates we are concerned with, that is, the user engagement metrics. Balancing, in this case, yields a set of `client_id` for Beta that resembles Release. This gives us an idea of how these users do indeed change in time. If we see changes that are larger than anticipated, then we know that something significant is happening in user engagement that we can "forecast" in the subsequent Release.

First, we determine the number of training (v67) Beta and Release clients that are in the validation set (v68).

```{r message=FALSE, warning=FALSE, echo=FALSE}
mutual_clients <- df_train_1x[, c('client_id', 'label')] %>% 
  inner_join(df_validate_encoder[, c('client_id', 'label')]) %>%
  count('label')

mutual_clients
```

Let's compare this to existing distribution:

```{r echo=FALSE}
total <- df_train_1x %>% count('label')
cat('Percentage of beta mutual clients:',round(mutual_clients$freq[1] * 100 / total$freq[1]),'%\n')
cat('Percentage of release mutual clients:',round(mutual_clients$freq[2] * 100 / total$freq[2]),'%')
```

Hence, most training clients (65%) are in the validation set.

## Holdout Covariates

* User engagement metrics:
  - active_hours
  - active_hours_max
  - uri_count
  - uri_count_max
  - search_count
  - search_count_max
  - num_pages
  - num_pages_max
  - daily_max_tabs
  - daily_max_tabs_max
  - daily_unique_domains
  - daily_unique_domains_max
  - daily_tabs_opened
  - daily_tabs_opened_max

Subset the validation clients down to those matched:

```{r echo=FALSE}
df_validate_matched <- df_validate_1x %>%
  filter(client_id %in% df_matched$client_id)

df_validate_matched %>%
  count('label')
```

### Training and Validation Difference:

```{r}
stats_pre <- calc_delta(df_validate_1x, engagement)
stats_post <- calc_delta(df_validate_matched, engagement)

stats_mean <- stats_pre[1, ] %>%
  rbind(stats_post[1, ]) %>%
  set_rownames(c('pre-matching', 'post-matching'))

stats_median <- stats_pre[2, ] %>%
  rbind(stats_post[2, ]) %>%
  set_rownames(c('pre-matching', 'post-matching'))
```

> Mean

```{r results = 'asis'}
knitr::kable(stats_mean) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  scroll_box(width = "100%")
```

> Median

```{r results = 'asis'}
knitr::kable(stats_median) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  scroll_box(width = "100%")
```

***

```{r fig.width=10, fig.height=50, echo=FALSE, results = 'asis'}
df_validate_full <- df_validate_matched %>%
  filter(label == 'beta') %>%
  mutate(label =  'beta - matched') %>%
  rbind(df_validate_1x) %>% 
  distinct()

stats <- calc_stats(df_validate_full, engagement) %>%
  dplyr::select('metric','label', everything())

knitr::kable(stats) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  row_spec(1:3, bold = T, color = "white", background = "#4D5686") %>%
  row_spec(4:6, bold = T, color = "white", background = "#C5578C") %>%
  scroll_box(width = "100%")
```

```{r message=FALSE, warning=FALSE}
plots <- list()

for (covariate in engagement) {
  stats_rel <- stats %>% filter(label == 'release') %>% dplyr::select(covariate, metric)
  means <- stats_rel[stats_rel$metric == 'mean', covariate]
  medians <- stats_rel[stats_rel$metric == 'median', covariate]
  ho_means <- stats %>% filter(label == 'beta' & metric == 'mean') %>% dplyr::select(covariate)
  
  plots[[covariate]] <- compare_log_cont(df_validate_full, covariate, means, medians, as.numeric(ho_means), print=FALSE) 
}
```


### Kolmogorov-Smirnov test (KS)

Once again, we use the KS test to verify whether any significant difference between the average user engagement metrics in the Beta and Release groups, over several versions (v67 and v68). *Reminder:* smaller KS distance values indicate a better balance.

```{r fig.height=9, fig.width=9, message=FALSE, warning=FALSE}
par(mfrow = c(4, 2))  

df_beta <- df_validate_full %>% filter(label == 'beta - matched')
df_rel <- df_validate_full %>% filter(label == 'release')

output = data.frame()

for (i in engagement) {
  
  # Training
  x_t <- df_beta[,i]
  y_t <- df_rel[,i]
  
  rg_t <- range(x_t, y_t, na.rm=T)
  ks <- ks.test(x_t, y_t)$statistic
  output = rbind(output, data.frame(KS = ks))
}
```

```{r message=FALSE, warning=FALSE, echo=FALSE, results = 'asis'}
kable(set_rownames(output, engagement)) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
scroll_box(width = "100%")
```

### Visual inspection 

```{r echo=FALSE, fig.height=4, fig.width=7}
par(mfrow = c(7, 2))
for (v in engagement){
  p <- ggplot(data=df_validate_full, aes(x = get(v), group = label, fill = label)) +
    geom_density(aes(fill = label), adjust=1.5, alpha=.4) + 
    scale_fill_manual(values = c("#111d5e", "#b21f66","#611e62")) +
    labs(y='Density', title=v) + theme_bw() + theme(legend.position="bottom") 
  print(p)
}
```

The following violin plots depicts distributions for the following subsets: 

* Beta v68: pre-matching
* Beta v68: matched and subsetted
* Release v68

**NOTE**: Guiding lines have been added for the following:

* <span style="color:black">black</span> solid: Release mean
* <span style="color:black">black</span> dashed: Release median
* <span style="color:#D62728B2">red</span> dashed line: subsetted Beta _mean_. 

```{r message=FALSE, warning=FALSE}
plots <- list()

for (covariate in engagement) {
  stats_rel <- stats %>% filter(label == 'release') %>% dplyr::select(covariate, metric)
  means <- stats_rel[stats_rel$metric == 'mean', covariate]
  medians <- stats_rel[stats_rel$metric == 'median', covariate]
  ho_means <- stats %>% filter(label == 'beta' & metric == 'mean') %>% dplyr::select(covariate)
  
  plots[[covariate]] <- compare_log_cont(df_validate_full, covariate, means, medians, as.numeric(ho_means), print=FALSE) 
}
```

```{r echo=FALSE, fig.height=50, fig.width=10, message=FALSE, warning=FALSE}
plot_grid(plotlist = plots, ncol = 1)
```

***
#### Observations

Our main objective was to determine if the user engagement metrics changed in the newest Beta version concerning the previous Release version. The density and violin plots show that there are significant differences between both groups (Beta and Release) concerning some user engagement metrics, listed as follows.

* `daily_max_tabs`
* `daily_max_tabs_max`
* `num_pages`
* `num_pages_max`
* `daily_unique_domains`

```{r eval=FALSE, include=FALSE}
# Saving
save.image(file = "model_validation.RData")
```