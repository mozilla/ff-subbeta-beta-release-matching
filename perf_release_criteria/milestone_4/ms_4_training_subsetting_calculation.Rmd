---
title: 'Milestone 4: Datasets Generation'
output:
  html_notebook:
    theme: cosmo
    toc: yes
    toc_float: yes
date: 'Last Updated: `r format(Sys.time(), "%B %d, %Y")`'
---

```{r imports, echo=FALSE, warning=FALSE, message=FALSE}
source('../lib/modeling.R')
source('../lib/supporting_funcs.R')
source('../lib/scoring.R')
library(bigrquery)
library(DBI)
library(lubridate)
```

# tl;dr 
This notebook generates the training (Version N) and prediction (Version N+1) datasets, trains a statistical matching model, extracts a representative subset of Beta with respect to Release, and calculates the final Release Health Metrics as described in milestone 4 of this PRD[Release performance health metrics](https://docs.google.com/document/d/1Ygz6MkudYHZjnDnD9Z97kUyFrvV3KGWsjXyPjddhHq0/edit#heading=h.lvb9l8gw2nee). 

# Dataset Preparation
The datasets are generated by this [notebook](
https://storage.cloud.google.com/moz-fx-data-bq-data-science-cdowhygelund/notebooks/jupyter/Beta_Release_Subsetting_Data_Preparation_GCP.ipynb
) on a Dataproc cluster. These are serialized as BigQuery tables available in the `moz-fx-data-shared-prod.analysis` dataset as `ccd_subBeta_training` (Version N) and `ccd_subBeta_prerelease` (Version N+1). 

# Covariate Generation
The following process is very similar to that used in the validation of the [proof-of-concept](https://metrics.mozilla.com/protected/cdowhygelund/beta_subset_release_validation.html). In addition:

* `na.omit` is not applied.
* Missing values are only dropped for the performance metrics (outcomes) of concern
* OS is filtered to only Windows. 

```{r gcp_connect, echo=FALSE}
project <- "moz-fx-dev-cdowhyglund-subbeta"

con <- dbConnect(
  bigrquery::bigquery(),
  project = 'moz-fx-data-shared-prod',
  dataset = 'analysis',
  billing = project
)
```

```{r var_defs, echo=FALSE}
# outcomes <- c('TIME_TO_DOM_INTERACTIVE_MS', 'TIME_TO_DOM_CONTENT_LOADED_END_MS', 'TIME_TO_LOAD_EVENT_END_MS', 
#                'TIME_TO_NON_BLANK_PAINT_MS', 'TIME_TO_DOM_COMPLETE_MS', 'FX_PAGE_LOAD_MS_2_PARENT')

outcomes <- names(get_m2_metric_map())
# outcomes <- c('TIME_TO_DOM_CONTENT_LOADED_END_MS', 'TIME_TO_LOAD_EVENT_END_MS', 'TIME_TO_DOM_COMPLETE_MS', 'FX_PAGE_LOAD_MS_2_PARENT') 

# define response field
label <- 'is_release'
```

```{r data_prep}
load_df <- function(table){
  df <- tbl(con, table) %>%
#     filter(locale %in% c('en-US', 'en-GB')) %>%  (FIXME)
    filter(country %in% c('US', 'GB')) %>%
    filter(os == 'Windows_NT') %>%
    collect() %>%
    # mutate(client_id = as.character(client_id)) %>%
    select(-os, country) %>%
    drop_na(outcomes) %>%
    mutate(default_search_engine = as.factor(normalize_search_engine(default_search_engine))) %>%
    mutate(profile_age_cat = as.factor(normalize_profile_age(profile_age))) %>%
    mutate(distro_id_norm = as.factor(normalize_distro_id(distribution_id))) %>%
    mutate(timezone_cat = as.factor(normalize_timezone(timezone_offset))) %>%
    mutate(memory_cat = as.factor(normalize_memory(memory_mb))) %>% 
    mutate(cpu_speed_cat = as.factor(normalize_cpu_speed(cpu_speed_mhz))) %>% 
    mutate(cpu_cores_cat = as.factor(normalize_cpu_cores(cpu_cores))) %>% 
    mutate(!!label := case_when(
      label == 'beta' ~ FALSE,
      TRUE ~ TRUE)) %>%
    filter(startup_ms >= 0 & profile_age >= 0)
}

df_train <- load_df("ccd_subBeta_training")
df_predict <- load_df('ccd_subBeta_prerelease')
```

# Review 

```{r summarize}
summary(df_train)
```

## Observations

These observations were noted previously during Milestone 2 (`ms_2_datasets_generation.Rmd`). Noting here now for completeness.
* Content crashes looks strange. Always is 0. Not sure what happened there...
    - Appears to always be 0 
    - **Action**: Remove it
* Categories for `cpu_vendor`, and `os_version` have several low populated categories
    - Remap these categories to "Other"
* `city`, `geo_subdivision1`, and  `geo_subdivision2` are dominated by "other" categories.
    - **Action**: Remove them
*  `cpu_l3_cache_kb` and `cpu_stepping` have large number of NA's
    - **Action**: Remove them
*  `cpu_model` and `cpu_family` appear to be subcategories of `cpu_family`.
    - **Action**: Remove for modeling. 
    - **However**: These could (and other cpu fields) be added in for use in validation and slicing/segmentation.
* 'distribution_id` is redundant with `distro_id_norm`
    - **Action**: Remove it
* `cpu_l2_cache_kb` has a small number of NA
    - **Action**: Drop these rows as they are very small fraction. 
    - **Action**: Create a categorical variable to reflect other environment covariates.
* `install_year` and `profile_age` have unrealistic values
    - **Action**: Filter them out. `install_year` between 1990 and now. Profiles younger than 20 years. 
* `startup_ms` has crazy times
    - **Action**: The top 0.001% are being filtered already. Review to determine how these change with matching on performance metrics.
* `fxa_configured` has beeen noted to be incorrectly calculated using `clients_daily`. This will be removed until a fix is in place from `main_summary`
    - **FIXME**: Update `main_summary` to pull the data correctly
* Drop unused factors on `default_search_engine`, `is_wow64` 


## Duplicate clients 
Find number of clients that are both in Beta and Release. This isn't a problem per se, it is just an interesting observation. 

```{r}
df_train %>% count(client_id) %>% select(n) %>% table
```

Duplicate cients are really uncommon!

## Cleaning

Performing the proposed actions above

```{r cleaning, echo=FALSE, message=FALSE, warning=FALSE}
normalize_os_version <- function(os_version){
  levels <- c('Other', '6.1', '6.2', '6.3', '10.0')
  factor(
    case_when(
      os_version == '10.0' ~ levels[5],
      os_version == '6.3' ~ levels[4],
      os_version == '6.2' ~ levels[3],
      os_version == '6.1' ~ levels[2],
      TRUE ~ levels[1]
    ), levels = levels, ordered = TRUE
  )
}

normalize_cpu_vendor <- function(cpu_vendor){
  factor(
    case_when(
      cpu_vendor == 'AuthenticAMD' ~ 'AMD',
      cpu_vendor == 'GenuineIntel' ~ 'Intel',
      TRUE ~ 'Other'
    )
  )
}

normalize_cpu_l2_cache <- function(cpu_l2_cache){
  levels <- c('< 256', '< 512', '< 768', '< 1024', '> 1024')
  factor(
    case_when(
      cpu_l2_cache <= 256 ~ levels[1],
      cpu_l2_cache <= 512 ~ levels[2],
      cpu_l2_cache <= 768 ~ levels[3],
      cpu_l2_cache <= 1024 ~ levels[4],
      TRUE ~ levels[5]
    )
  )
}

stage_2_clean <- function(df){
  df_clean <- df %>%
    select(-c(city, geo_subdivision1, geo_subdivision2, cpu_l3_cache_kb, cpu_stepping, 
              cpu_model, cpu_family, distribution_id, fxa_configured)) %>%
    filter(between(install_year, 1991, 2019)) %>%
    filter(profile_age <= 365 * 20 ) %>%
    na.omit() %>% 
    mutate(os_version = normalize_os_version(os_version)) %>%
    mutate(cpu_vendor = normalize_cpu_vendor(cpu_vendor)) %>%
    mutate(cpu_l2_cache_kb_cat = normalize_cpu_l2_cache(cpu_l2_cache_kb)) %>%
    droplevels() %>%
    mutate(data_created = now()) # Add in a field for when this notebook was run
  return(df_clean)
}


# Add in a field for when the training and prerelease datasets were created 
df_train_f <- stage_2_clean(df_train) %>%
    mutate(date_created = now())

df_predict_f <- stage_2_clean(df_predict) %>%
    mutate(date_created = now())
```


```{r}
summary(df_train_f)
```

# Modeling Dataset

The optimal modeling dataset utilized an 4x oversampling of Beta with respect to Release. 

```{r training_oversample}
oversample <- function(df_c, oversampling){
  df_beta <- df_c %>% filter(is_release == FALSE)
  df_rel <- df_c %>% filter(is_release == TRUE)
  n_beta <- nrow(df_beta)
  
  model_ids <- df_rel %>%
    sample_n(size = round(n_beta / oversampling)) %>%
    rbind(df_beta) %>%
    pull(client_id)
  
  return(df_c %>% filter(client_id %in% model_ids))
}

df_train_x <- oversample(df_train_f, oversampling = 4)
```

# Train

The statistical matching model architecture utilized was determined in [ms_2_validation](https://metrics.mozilla.com/protected/cdowhygelund/subBeta_m3_model_determination.html). 

Model framework: 

* matching algorithm: Nearest-Neighbors Probit, Linear
* features: Only performance metrics
* hyperparameters: 
    * `caliper` = 0.4
    * `calcloset` = FALSE
    * `ratio` = 3
    * `replace` = FALSE
    * 4x oversampling
    * Interactions across covariates
    
```{r train, message=FALSE, error=FALSE}
model <- train_matchit(df_train_x, outcomes, add_interactions = TRUE, replace = FALSE, 
                         caliper = 0.4, calclosest = TRUE, ratio = 3, distance = "linear.probit")
```


Subset Beta for the representative dataset

```{r beta_subset}
predictions <- extract_predictions(model$matched, df_predict_f) 
```

# Release Health Metrics

Calculate the Release Health Metrics:
**TBD**

# Serialize and push to GCP
Serialize the following:


* Cleaned training and prerelease datasets
* Statistical matching model
* Subsetted prerelease dataset for performance calculation
* Prerelease performance results 

## Storage
Save the resultant dataset and bootstrap samples to an R image. 

```{r serialize_datasets}
# training datasets
train_file_name <- paste('df_train_', format(Sys.time(), "%Y%m%d"), '.RData', sep='')
train_file_path <- file.path('data', train_file_name)
save(df_train_f, df_predict_f, df_train_x, model, file = train_file_path)
gcs_train_file_path <- file.path('data', 'milestone4', train_file_name)

# final prelease subsetted dataset
prerelease_file_name <- paste('df_prerelease_', format(Sys.time(), "%Y%m%d"), '.RData', sep='')
prerelease_file_path <- file.path('data', prerelease_file_name)
save(predictions, file = prerelease_file_path)
gcs_prerelease_file_path <- file.path('data', 'milestone4', prerelease_file_name)

# Release health metrics
release_health_file_name <- paste('df_release_health_', format(Sys.time(), "%Y%m%d"), '.RData', sep='')
release_health_file_path <- file.path('data', release_health_file_name)
# save(df_release_health_f, df_predict_f, df_release_health_x, file = release_health_file_path)
gcs_release_health_file_path <- file.path('data', 'milestone4', release_health_file_name)
```

Then push the results to GCP in the `moz-fx-dev-subbeta` bucket, to the following files:

* `r gcs_train_file_path`
* `r gcs_prerelease_file_path`
* `r gcs_release_health_file_path`


```{r gcp_train_storage_push, warning=FALSE}
Sys.setenv("GCS_DEFAULT_BUCKET" = "moz-fx-dev-subbeta",
           "GCS_AUTH_FILE" = "~/certs/moz-fx-dev-cdowhyglund-subBeta-788f8f0d4627.json")

library(googleCloudStorageR)

upload_try <- gcs_upload(train_file_path, name = gcs_train_file_path)
upload_try
```

```{r gcp_prelease_storage_push, warning=FALSE}
upload_try <- gcs_upload(prerelease_file_path, name = gcs_prerelease_file_path)
upload_try
```

```{r gcp_health_metrics_storage_push, warning=FALSE}
# upload_try <- gcs_upload(gcs_release_health_file_path, name = gcs_file_path)
# upload_try
```

## BigQuery

Training and prerelease datasets
```{r big_query_write}
project <- "moz-fx-dev-cdowhyglund-subBeta"
# project <- "moz-fx-data-bq-data-science"

bq_table(project = 'moz-fx-dev-cdowhyglund-subbeta',
         dataset = "training",
         table   = "ccd_subbeta_training_cleaned") %>%
  bq_table_upload(values = df_train_x,
                  create_disposition = "CREATE_IF_NEEDED",
                  write_disposition = "WRITE_APPEND",
                  fields = as_bq_fields(df_train_x) )

bq_table(project = 'moz-fx-dev-cdowhyglund-subbeta',
         dataset = "training",
         table   = "ccd_subbeta_training_final") %>%
  bq_table_upload(values = df_train_x,
                  create_disposition = "CREATE_IF_NEEDED",
                  write_disposition = "WRITE_APPEND",
                  fields = as_bq_fields(df_train_x) )

bq_table(project = 'moz-fx-dev-cdowhyglund-subbeta',
         dataset = "prerelease",
         table   = "ccd_subbeta_prelease_cleaned") %>%
  bq_table_upload(values = df_predict_f,
                  create_disposition = "CREATE_IF_NEEDED",
                  write_disposition = "WRITE_APPEND",
                  fields = as_bq_fields(df_predict_f) )

```


Subsetted dataset
```{r bq_write_subset}
bq_table(project = 'moz-fx-dev-cdowhyglund-subbeta',
         dataset = "prerelease",
         table   = "ccd_subbeta_prelease_subset") %>%
  bq_table_upload(values = df_predict_f,
                  create_disposition = "CREATE_IF_NEEDED",
                  write_disposition = "WRITE_APPEND",
                  fields = as_bq_fields(df_predict_f) )
```

Release health Metrics
```{r bq_release_health_metrics}
# bq_table(project = 'moz-fx-dev-cdowhyglund-subbeta',
#          dataset = "prerelease",
#          table   = "ccd_subbeta_prerelease_health_metrics") %>%
#   bq_table_upload(values = release_health_df,
#                   create_disposition = "CREATE_IF_NEEDED",
#                   write_disposition = "WRITE_APPEND",
#                   fields = as_bq_fields(release_health_df) )
```
