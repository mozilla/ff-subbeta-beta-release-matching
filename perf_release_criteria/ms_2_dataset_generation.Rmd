---
title: 'Milestone 2: Datasets Generation'
date: 'Last Updated: `r format(Sys.time(), "%B %d, %Y")`'
output: 
  html_document:
    theme: cosmo
    toc: true
    toc_float: true
---

```{r imports, echo=FALSE, warning=FALSE, message=FALSE}
source('lib/supporting_funcs.R')
```

# tl;dr 
This notebook generates the training and validation datasets used to determine the model framework for this [PRD](https://docs.google.com/document/d/1Ygz6MkudYHZjnDnD9Z97kUyFrvV3KGWsjXyPjddhHq0/edit#heading=h.lvb9l8gw2nee). These datasets will be used for all steps in the process detailed out in the [document](https://docs.google.com/document/d/1SfuanvmYmvmEFAdQ7Z5djDeLezdNB1TESqVmj93O8to/edit#heading=h.nwwrif453n80). 

# Dataset Preparation
The datasets were generated by this [notebook](https://dbc-caf9527b-e073.cloud.databricks.com/#notebook/210500/command/210501). The files were manually downloaded from S3 on hala. 

# Covariate Generation
The following process is very similar to that used in the validation of the [proof-of-concept](https://metrics.mozilla.com/protected/cdowhygelund/beta_subset_release_validation.html). In addition:


* `na.omit` is not applied.
* OS is filtered to only Windows. 

```{r var_defs, echo=FALSE}
outcomes <- c('TIME_TO_DOM_INTERACTIVE_MS', 'TIME_TO_DOM_CONTENT_LOADED_END_MS', 'TIME_TO_LOAD_EVENT_END_MS', 
              'TIME_TO_NON_BLANK_PAINT_MS', 'TIME_TO_DOM_COMPLETE_MS', 'FX_PAGE_LOAD_MS_2_PARENT')
```

```{r data_prep}
# define response field
label <- 'is_release'

load_df <- function(file_path){
  df <- read.csv(file_path) %>%
    filter(locale %in% c('en-US', 'en-GB')) %>%
    filter(country %in% c('US', 'GB')) %>%
    filter(os == 'Windows_NT') %>%
    select(-os, country, locale) %>%
    drop_na(outcomes) %>%
    mutate(default_search_engine = as.factor(normalize_search_engine(default_search_engine))) %>%
    mutate(profile_age_cat = as.factor(normalize_profile_age(profile_age))) %>%
    mutate(distro_id_norm = as.factor(normalize_distro_id(distribution_id))) %>%
    mutate(timezone_cat = as.factor(normalize_timezone(timezone_offset))) %>%
    mutate(memory_cat = as.factor(normalize_memory(memory_mb))) %>% 
    mutate(cpu_speed_cat = as.factor(normalize_cpu_speed(cpu_speed_mhz))) %>% 
    mutate(cpu_cores_cat = as.factor(normalize_cpu_cores(cpu_cores))) %>% 
    mutate(!!label := case_when(
      label == 'beta' ~ FALSE,
      TRUE ~ TRUE)) %>%
    filter(startup_ms >= 0 & profile_age >= 0)
}

df_train <- load_df('data/milestone2/df_pd_67.csv')
df_validate <- load_df('data/milestone2/df_pd_68.csv')
```

# Review 

```{r summarize}
summary(df_train)
```

## Observations

* Content crashes looks strange. Always is 0. Not sure what happened there...
    - Appears to always be 0 
    - **Action**: Remove it
* Categories for `cpu_vendor`, and `os_version` have several low populated categories
    - Remap these categories to "Other"
* `city`, `geo_subdivision1`, and  `geo_subdivision2` are dominated by "other" categories.
    - **Action**: Remove them
*  `cpu_l3_cache_kb` and `cpu_stepping` have large number of NA's
    - **Action**: Remove them
*  `cpu_model` and `cpu_family` appear to be subcategories of `cpu_family`.
    - **Action**: Remove for modeling. 
    - **However**: These could (and other cpu fields) be added in for use in validation and slicing/segmentation.
* 'distribution_id` is redundant with `distro_id_norm`
    - **Action**: Remove it
* `cpu_l2_cache_kb` has a small number of NA
    - **Action**: Drop these rows as they are very small fraction. 
    - **Action**: Create a categorical variable to reflect other environment covariates.
* `install_year` and `profile_age` have unrealistic values
    - **Action**: Filter them out. `install_year` between 1990 and now. Profiles younger than 20 years. 
* `startup_ms` has crazy times
    - **Action**: The top 0.001% are being filtered already. Review to determine how these change with matching on performance metrics.
* Drop unused factors on `default_search_engine`, `is_wow64` 


## Duplicate clients 
Find number of clients that are both in Beta and Release. This isn't a problem per se, it is just an interesting observation. 

```{r}
df_train %>% count(client_id) %>% select(n) %>% table
```

Duplicate cients are really uncommon!

## Cleaning

Performing the proposed actions above

```{r cleaning, echo=FALSE, message=FALSE, warning=FALSE}
normalize_os_version <- function(os_version){
  levels <- c('Other', '6.1', '6.2', '6.3', '10.0')
  factor(
    case_when(
      os_version == '10.0' ~ levels[5],
      os_version == '6.3' ~ levels[4],
      os_version == '6.2' ~ levels[3],
      os_version == '6.1' ~ levels[2],
      TRUE ~ levels[1]
    ), levels = levels, ordered = TRUE
  )
}

normalize_cpu_vendor <- function(cpu_vendor){
  factor(
    case_when(
      cpu_vendor == 'AuthenticAMD' ~ 'AMD',
      cpu_vendor == 'GenuineIntel' ~ 'Intel',
      TRUE ~ 'Other'
    )
  )
}

normalize_cpu_l2_cache <- function(cpu_l2_cache){
  levels <- c('< 256', '< 512', '< 768', '< 1024', '> 1024')
  factor(
    case_when(
      cpu_l2_cache <= 256 ~ levels[1],
      cpu_l2_cache <= 512 ~ levels[2],
      cpu_l2_cache <= 768 ~ levels[3],
      cpu_l2_cache <= 1024 ~ levels[4],
      TRUE ~ levels[5]
    )
  )
}

stage_2_clean <- function(df){
  df_clean <- df %>%
    select(-c(city, geo_subdivision1, geo_subdivision2, cpu_l3_cache_kb, cpu_stepping, 
              cpu_model, cpu_family, distribution_id)) %>%
    filter(between(install_year, 1991, 2019)) %>%
    filter(profile_age <= 365 * 20 ) %>%
    na.omit() %>% 
    mutate(os_version = normalize_os_version(os_version)) %>%
    mutate(cpu_vendor = normalize_cpu_vendor(cpu_vendor)) %>%
    mutate(cpu_l2_cache_kb_cat = normalize_cpu_l2_cache(cpu_l2_cache_kb)) %>%
    droplevels()
  return(df_clean)
}

df_train_f <- stage_2_clean(df_train)
df_validate_f <- stage_2_clean(df_validate)

```

```{r}
summary(df_train_f)
```

# Serialize and push to GCP
Save the result dataset to an R image. 

```{r serialize}
image_file_path <- paste('data/milestone2/df_train_validate_', format(Sys.time(), "%Y%m%d"), '.RData', sep='')
save(df_train_f, df_validate_f, file = image_file_path)
```

The push the results to GCP in the `moz-fx-dev-subbeta` bucket, under `r image_file_path`.

```{r gcp_storage_push, warning=FALSE}
Sys.setenv("GCS_DEFAULT_BUCKET" = "moz-fx-dev-subbeta",
           "GCS_AUTH_FILE" = "moz-fx-dev-cdowhyglund-subBeta-788f8f0d4627.json")

library(googleCloudStorageR)

upload_try <- gcs_upload(image_file_path, name = image_file_path)
upload_try
```


