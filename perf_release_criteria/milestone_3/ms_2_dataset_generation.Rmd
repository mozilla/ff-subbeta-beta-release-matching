---
title: 'Milestone 2: Datasets Generation'
output:
  html_document:
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
date: 'Last Updated: `r format(Sys.time(), "%B %d, %Y")`'
---

```{r imports, echo=FALSE, warning=FALSE, message=FALSE}
source('../lib/supporting_funcs.R')
```

# tl;dr 
This notebook generates the training and validation datasets used to determine the model framework for this [PRD](https://docs.google.com/document/d/1Ygz6MkudYHZjnDnD9Z97kUyFrvV3KGWsjXyPjddhHq0/edit#heading=h.lvb9l8gw2nee). These datasets will be used for all steps in the process detailed out in the [document](https://docs.google.com/document/d/1SfuanvmYmvmEFAdQ7Z5djDeLezdNB1TESqVmj93O8to/edit#heading=h.nwwrif453n80). In addition, the bootstrap replicates for feature selection and hyperparameter tuning are created. These datasets are all stored in GCP under the `moz-fx-dev-cdowhyglund-subBeta` project.  

# Dataset Preparation
The datasets were generated by this [notebook](https://dbc-caf9527b-e073.cloud.databricks.com/#notebook/210500/command/210501). The files were manually downloaded from S3 on hala. 

# Covariate Generation
The following process is very similar to that used in the validation of the [proof-of-concept](https://metrics.mozilla.com/protected/cdowhygelund/beta_subset_release_validation.html). In addition:

* `na.omit` is not applied.
* Missing values are only dropped for the performance metrics (outcomes) of concern
* OS is filtered to only Windows. 

```{r var_defs, echo=FALSE}
outcomes <- c('TIME_TO_DOM_INTERACTIVE_MS', 'TIME_TO_DOM_CONTENT_LOADED_END_MS', 'TIME_TO_LOAD_EVENT_END_MS', 
              'TIME_TO_NON_BLANK_PAINT_MS', 'TIME_TO_DOM_COMPLETE_MS', 'FX_PAGE_LOAD_MS_2_PARENT')
```

```{r data_prep}
# define response field
label <- 'is_release'

load_df <- function(file_path){
  df <- read.csv(file_path) %>%
    mutate(client_id = as.character(client_id)) %>%
    filter(locale %in% c('en-US', 'en-GB')) %>%
    filter(country %in% c('US', 'GB')) %>%
    filter(os == 'Windows_NT') %>%
    select(-os, country, locale) %>%
    drop_na(outcomes) %>%
    mutate(default_search_engine = as.factor(normalize_search_engine(default_search_engine))) %>%
    mutate(profile_age_cat = as.factor(normalize_profile_age(profile_age))) %>%
    mutate(distro_id_norm = as.factor(normalize_distro_id(distribution_id))) %>%
    mutate(timezone_cat = as.factor(normalize_timezone(timezone_offset))) %>%
    mutate(memory_cat = as.factor(normalize_memory(memory_mb))) %>% 
    mutate(cpu_speed_cat = as.factor(normalize_cpu_speed(cpu_speed_mhz))) %>% 
    mutate(cpu_cores_cat = as.factor(normalize_cpu_cores(cpu_cores))) %>% 
    mutate(!!label := case_when(
      label == 'beta' ~ FALSE,
      TRUE ~ TRUE)) %>%
    filter(startup_ms >= 0 & profile_age >= 0)
}

df_train <- load_df('data/df_pd_67.csv')
df_validate <- load_df('data/df_pd_68.csv')
```

# Review 

```{r summarize}
summary(df_train)
```

## Observations

* Content crashes looks strange. Always is 0. Not sure what happened there...
    - Appears to always be 0 
    - **Action**: Remove it
* Categories for `cpu_vendor`, and `os_version` have several low populated categories
    - Remap these categories to "Other"
* `city`, `geo_subdivision1`, and  `geo_subdivision2` are dominated by "other" categories.
    - **Action**: Remove them
*  `cpu_l3_cache_kb` and `cpu_stepping` have large number of NA's
    - **Action**: Remove them
*  `cpu_model` and `cpu_family` appear to be subcategories of `cpu_family`.
    - **Action**: Remove for modeling. 
    - **However**: These could (and other cpu fields) be added in for use in validation and slicing/segmentation.
* 'distribution_id` is redundant with `distro_id_norm`
    - **Action**: Remove it
* `cpu_l2_cache_kb` has a small number of NA
    - **Action**: Drop these rows as they are very small fraction. 
    - **Action**: Create a categorical variable to reflect other environment covariates.
* `install_year` and `profile_age` have unrealistic values
    - **Action**: Filter them out. `install_year` between 1990 and now. Profiles younger than 20 years. 
* `startup_ms` has crazy times
    - **Action**: The top 0.001% are being filtered already. Review to determine how these change with matching on performance metrics.
* Drop unused factors on `default_search_engine`, `is_wow64` 


## Duplicate clients 
Find number of clients that are both in Beta and Release. This isn't a problem per se, it is just an interesting observation. 

```{r}
df_train %>% count(client_id) %>% select(n) %>% table
```

Duplicate cients are really uncommon!

## Cleaning

Performing the proposed actions above

```{r cleaning, echo=FALSE, message=FALSE, warning=FALSE}
normalize_os_version <- function(os_version){
  levels <- c('Other', '6.1', '6.2', '6.3', '10.0')
  factor(
    case_when(
      os_version == '10.0' ~ levels[5],
      os_version == '6.3' ~ levels[4],
      os_version == '6.2' ~ levels[3],
      os_version == '6.1' ~ levels[2],
      TRUE ~ levels[1]
    ), levels = levels, ordered = TRUE
  )
}

normalize_cpu_vendor <- function(cpu_vendor){
  factor(
    case_when(
      cpu_vendor == 'AuthenticAMD' ~ 'AMD',
      cpu_vendor == 'GenuineIntel' ~ 'Intel',
      TRUE ~ 'Other'
    )
  )
}

normalize_cpu_l2_cache <- function(cpu_l2_cache){
  levels <- c('< 256', '< 512', '< 768', '< 1024', '> 1024')
  factor(
    case_when(
      cpu_l2_cache <= 256 ~ levels[1],
      cpu_l2_cache <= 512 ~ levels[2],
      cpu_l2_cache <= 768 ~ levels[3],
      cpu_l2_cache <= 1024 ~ levels[4],
      TRUE ~ levels[5]
    )
  )
}

stage_2_clean <- function(df){
  df_clean <- df %>%
    select(-c(city, geo_subdivision1, geo_subdivision2, cpu_l3_cache_kb, cpu_stepping, 
              cpu_model, cpu_family, distribution_id)) %>%
    filter(between(install_year, 1991, 2019)) %>%
    filter(profile_age <= 365 * 20 ) %>%
    na.omit() %>% 
    mutate(os_version = normalize_os_version(os_version)) %>%
    mutate(cpu_vendor = normalize_cpu_vendor(cpu_vendor)) %>%
    mutate(cpu_l2_cache_kb_cat = normalize_cpu_l2_cache(cpu_l2_cache_kb)) %>%
    droplevels()
  return(df_clean)
}

df_train_f <- stage_2_clean(df_train)
df_validate_f <- stage_2_clean(df_validate)

```

```{r}
summary(df_train_f)
```

# Bootstrap Samples

```{r balancing, echo=FALSE}
channel_counts <- df_train_f %>% count(label)
release_overcount <- channel_counts$n[channel_counts$label == 'release'] - channel_counts$n[channel_counts$label == 'beta']
```

The feature selection will utilize 10 boostrap replicates, which are generated as follows:
1. The entire `df_train` dataset is sampled at 100% with replacement.
2. A sample of Release (`is_release = TRUE`) `client_id` are taken of the same size as Beta (`r channel_counts$n[channel_counts$label == 'beta']`)
3. The entire (resampled) Beta `client_id` are combined with this sampled Release ids. 
    - These are the profiles that will be used for building the matching model. 
    - The matched Beta profiles will be used for scoring the model. 
4. The remaining Release ids will be used for scoring the model. 

The same set of bootstrap samples will be used for all feature selection across modeling algorithms. A suite of these are created to represent different levels of Beta oversampling with respect to Release. 

```{r bootstrap_def}
extract_ids <- function(df_c, oversampling){
  df_beta <- df_c %>% filter(is_release == FALSE)
  df_rel <- df_c %>% filter(is_release == TRUE)
  n_beta <- nrow(df_beta)
  
  model_ids <- df_rel %>%
    sample_n(size = round(n_beta / oversampling)) %>%
    rbind(df_beta) %>%
    pull(client_id)
  
  # create the dataset for scoring matching model
  test_ids <- df_rel %>% 
    filter(!client_id %in% model_ids) %>%
    pull(client_id)
  
  return(list(train = model_ids, test = test_ids))
}

gen_bootstraps <- function(df_train, oversampling = 1, num_bts = 10){
  bts <- list()
  for (i in 1:num_bts){
    # create a resampled version of entire dataframe
    df_resampled <- df_train %>% sample_frac(replace = TRUE)
    # create train/test "splits"
    bts[[i]] <- extract_ids(df_resampled, oversampling = oversampling)
  }
  return(bts)
}
```

```{r bootstrap_gen}
bts.1x <- gen_bootstraps(df_train_f)
bts.4x <- gen_bootstraps(df_train_f, oversampling = 4)
bts.8x <- gen_bootstraps(df_train_f, oversampling = 8)
bts.16x <- gen_bootstraps(df_train_f, oversampling = 16)
```


# Serialize and push to GCP
Save the resultant dataset and bootstrap samples to an R image. 

```{r serialize_dataset}
image_file_name <- paste('df_train_validate_', format(Sys.time(), "%Y%m%d"), '.RData', sep='')
image_file_path <- file.path('data', image_file_name)
save(df_train_f, df_validate_f, bts.1x, bts.4x, bts.8x, bts.16x,
     file = image_file_path)

gcs_file_path <- file.path('data', 'milestone2', image_file_name)
```

Then push the results to GCP in the `moz-fx-dev-subbeta` bucket, under `r gcs_file_path`.

```{r gcp_storage_push, warning=FALSE}
Sys.setenv("GCS_DEFAULT_BUCKET" = "moz-fx-dev-subbeta",
           "GCS_AUTH_FILE" = "moz-fx-dev-cdowhyglund-subBeta-788f8f0d4627.json")

library(googleCloudStorageR)

upload_try <- gcs_upload(image_file_path, name = gcs_file_path)
upload_try
```


